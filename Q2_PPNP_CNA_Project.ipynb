{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-Z0OlTh2hBW",
        "outputId": "72f94346-c834-4223-e42b-ad9247678566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "# بارگزاری داده‌ها و چاپ آمار دیتاست\n",
        "def load_and_print_stats(dataset_name):\n",
        "    dataset = Planetoid(root=f'/tmp/{dataset_name}', name=dataset_name)\n",
        "    data = dataset[0]\n",
        "    print(f\"{dataset_name} Dataset Statistics:\")\n",
        "    print(f\"Number of Nodes: {data.num_nodes}\")\n",
        "    print(f\"Number of Edges: {data.num_edges}\")\n",
        "    print(f\"Number of Classes: {dataset.num_classes}\")\n",
        "    print(f\"Number of Node Features: {data.num_node_features}\")\n",
        "    return data, dataset.num_classes\n",
        "\n",
        "# تقسیم داده‌ها به آموزش، اعتبارسنجی و تست\n",
        "def split_data(data, dataset_name, train_ratio=0.7, val_ratio=0.1):\n",
        "    num_nodes = data.num_nodes\n",
        "    indices = np.random.permutation(num_nodes)\n",
        "\n",
        "    train_size = int(num_nodes * train_ratio)\n",
        "    val_size = int(num_nodes * val_ratio)\n",
        "\n",
        "    data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "    data.train_mask[indices[:train_size]] = True\n",
        "    data.val_mask[indices[train_size:train_size + val_size]] = True\n",
        "    data.test_mask[indices[train_size + val_size:]] = True\n",
        "\n",
        "    # چاپ تعداد داده‌ها در هر بخش\n",
        "    print(f\"Data Split for {dataset_name}:\")\n",
        "    print(f\"Training Nodes: {data.train_mask.sum().item()}\")\n",
        "    print(f\"Validation Nodes: {data.val_mask.sum().item()}\")\n",
        "    print(f\"Test Nodes: {data.test_mask.sum().item()}\\n\")\n",
        "\n",
        "# بارگزاری داده‌های Cora و CiteSeer و چاپ آمارها\n",
        "cora_data, cora_num_classes = load_and_print_stats('Cora')\n",
        "citeseer_data, citeseer_num_classes = load_and_print_stats('CiteSeer')\n",
        "\n",
        "# تقسیم داده‌های Cora و CiteSeer با اضافه کردن نام دیتاست به تابع\n",
        "split_data(cora_data, 'Cora', 0.7, 0.1)\n",
        "split_data(citeseer_data, 'CiteSeer', 0.7, 0.1)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PthrwGb03CF7",
        "outputId": "8f6ae6da-a52f-4009-bb9a-41b259237432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cora Dataset Statistics:\n",
            "Number of Nodes: 2708\n",
            "Number of Edges: 10556\n",
            "Number of Classes: 7\n",
            "Number of Node Features: 1433\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CiteSeer Dataset Statistics:\n",
            "Number of Nodes: 3327\n",
            "Number of Edges: 9104\n",
            "Number of Classes: 6\n",
            "Number of Node Features: 3703\n",
            "Data Split for Cora:\n",
            "Training Nodes: 1895\n",
            "Validation Nodes: 270\n",
            "Test Nodes: 543\n",
            "\n",
            "Data Split for CiteSeer:\n",
            "Training Nodes: 2328\n",
            "Validation Nodes: 332\n",
            "Test Nodes: 667\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPNP"
      ],
      "metadata": {
        "id": "r8oIr7mvCFdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import GCNConv, APPNP\n",
        "\n",
        "# Data loading and preprocessing\n",
        "def load_and_print_stats(dataset_name):\n",
        "    dataset = Planetoid(root=f'/tmp/{dataset_name}', name=dataset_name)\n",
        "    data = dataset[0]\n",
        "    print(f\"{dataset_name} Dataset Statistics:\")\n",
        "    print(f\"Number of Nodes: {data.num_nodes}\")\n",
        "    print(f\"Number of Edges: {data.num_edges}\")\n",
        "    print(f\"Number of Classes: {dataset.num_classes}\")\n",
        "    print(f\"Number of Node Features: {data.num_node_features}\")\n",
        "    return data, dataset.num_classes\n",
        "\n",
        "def split_data(data, dataset_name, train_ratio=0.7, val_ratio=0.1):\n",
        "    num_nodes = data.num_nodes\n",
        "    indices = np.random.permutation(num_nodes)\n",
        "\n",
        "    train_size = int(num_nodes * train_ratio)\n",
        "    val_size = int(num_nodes * val_ratio)\n",
        "\n",
        "    data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "    data.train_mask[indices[:train_size]] = True\n",
        "    data.val_mask[indices[train_size:train_size + val_size]] = True\n",
        "    data.test_mask[indices[train_size + val_size:]] = True\n",
        "\n",
        "    print(f\"Data Split for {dataset_name}:\")\n",
        "    print(f\"Training Nodes: {data.train_mask.sum().item()}\")\n",
        "    print(f\"Validation Nodes: {data.val_mask.sum().item()}\")\n",
        "    print(f\"Test Nodes: {data.test_mask.sum().item()}\\n\")\n",
        "\n",
        "# Model definition\n",
        "class PPNP(torch.nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(PPNP, self).__init__()\n",
        "        self.conv1 = GCNConv(num_features, 16)\n",
        "        self.conv2 = GCNConv(16, num_classes)\n",
        "        self.propagation = APPNP(K=10, alpha=0.1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.propagation(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train(model, data, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(model, data):\n",
        "    model.eval()\n",
        "    out = model(data)\n",
        "    _, pred = out.max(dim=1)\n",
        "    correct = pred[data.test_mask].eq(data.y[data.test_mask]).sum().item()\n",
        "    acc = correct / int(data.test_mask.sum())\n",
        "    return acc\n",
        "\n",
        "# Load data and split\n",
        "cora_data, cora_num_classes = load_and_print_stats('Cora')\n",
        "citeseer_data, citeseer_num_classes = load_and_print_stats('CiteSeer')\n",
        "split_data(cora_data, 'Cora', 0.7, 0.1)\n",
        "split_data(citeseer_data, 'CiteSeer', 0.7, 0.1)\n",
        "\n",
        "# Initialize models and optimizers\n",
        "cora_model = PPNP(num_features=cora_data.num_node_features, num_classes=cora_num_classes)\n",
        "citeseer_model = PPNP(num_features=citeseer_data.num_node_features, num_classes=citeseer_num_classes)\n",
        "cora_optimizer = torch.optim.Adam(cora_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "citeseer_optimizer = torch.optim.Adam(citeseer_model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "# Training loops\n",
        "\n",
        "# Training loop for Cora dataset\n",
        "for epoch in range(200):\n",
        "    cora_loss = train(cora_model, cora_data, cora_optimizer)\n",
        "    cora_train_acc = evaluate(cora_model, cora_data)  # Evaluating on the entire dataset\n",
        "    print(f'Cora - Epoch: {epoch+1}, Loss: {cora_loss:.4f}, Training Acc: {cora_train_acc:.4f}')\n",
        "\n",
        "# Training loop for CiteSeer dataset\n",
        "for epoch in range(200):\n",
        "    citeseer_loss = train(citeseer_model, citeseer_data, citeseer_optimizer)\n",
        "    citeseer_train_acc = evaluate(citeseer_model, citeseer_data)  # Evaluating on the entire dataset\n",
        "    print(f'CiteSeer - Epoch: {epoch+1}, Loss: {citeseer_loss:.4f}, Training Acc: {citeseer_train_acc:.4f}')\n",
        "\n",
        "\n",
        "# # Evaluate on test set after training\n",
        "# cora_test_acc = evaluate(cora_model, cora_data)\n",
        "# print(f'\\nCora - Test Accuracy: {cora_test_acc:.4f}')\n",
        "\n",
        "# # Evaluate on test set after training\n",
        "# citeseer_test_acc = evaluate(citeseer_model, citeseer_data)\n",
        "# print(f'\\nCiteSeer - Test Accuracy: {citeseer_test_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvdZCokfN1f2",
        "outputId": "944884b3-5572-4a02-90d4-9b373b93ec96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cora Dataset Statistics:\n",
            "Number of Nodes: 2708\n",
            "Number of Edges: 10556\n",
            "Number of Classes: 7\n",
            "Number of Node Features: 1433\n",
            "CiteSeer Dataset Statistics:\n",
            "Number of Nodes: 3327\n",
            "Number of Edges: 9104\n",
            "Number of Classes: 6\n",
            "Number of Node Features: 3703\n",
            "Data Split for Cora:\n",
            "Training Nodes: 1895\n",
            "Validation Nodes: 270\n",
            "Test Nodes: 543\n",
            "\n",
            "Data Split for CiteSeer:\n",
            "Training Nodes: 2328\n",
            "Validation Nodes: 332\n",
            "Test Nodes: 667\n",
            "\n",
            "Cora - Epoch: 1, Loss: 1.9580, Training Acc: 0.2505\n",
            "Cora - Epoch: 2, Loss: 1.8788, Training Acc: 0.3131\n",
            "Cora - Epoch: 3, Loss: 1.8124, Training Acc: 0.2983\n",
            "Cora - Epoch: 4, Loss: 1.7435, Training Acc: 0.3223\n",
            "Cora - Epoch: 5, Loss: 1.6680, Training Acc: 0.3923\n",
            "Cora - Epoch: 6, Loss: 1.5976, Training Acc: 0.4880\n",
            "Cora - Epoch: 7, Loss: 1.5236, Training Acc: 0.5525\n",
            "Cora - Epoch: 8, Loss: 1.4430, Training Acc: 0.5856\n",
            "Cora - Epoch: 9, Loss: 1.3865, Training Acc: 0.6298\n",
            "Cora - Epoch: 10, Loss: 1.3198, Training Acc: 0.6630\n",
            "Cora - Epoch: 11, Loss: 1.2621, Training Acc: 0.7109\n",
            "Cora - Epoch: 12, Loss: 1.1884, Training Acc: 0.7422\n",
            "Cora - Epoch: 13, Loss: 1.1424, Training Acc: 0.7459\n",
            "Cora - Epoch: 14, Loss: 1.0868, Training Acc: 0.7532\n",
            "Cora - Epoch: 15, Loss: 1.0254, Training Acc: 0.7661\n",
            "Cora - Epoch: 16, Loss: 0.9607, Training Acc: 0.7772\n",
            "Cora - Epoch: 17, Loss: 0.9118, Training Acc: 0.7901\n",
            "Cora - Epoch: 18, Loss: 0.8608, Training Acc: 0.7919\n",
            "Cora - Epoch: 19, Loss: 0.8194, Training Acc: 0.8029\n",
            "Cora - Epoch: 20, Loss: 0.7714, Training Acc: 0.8158\n",
            "Cora - Epoch: 21, Loss: 0.7213, Training Acc: 0.8287\n",
            "Cora - Epoch: 22, Loss: 0.6958, Training Acc: 0.8379\n",
            "Cora - Epoch: 23, Loss: 0.6363, Training Acc: 0.8435\n",
            "Cora - Epoch: 24, Loss: 0.6270, Training Acc: 0.8435\n",
            "Cora - Epoch: 25, Loss: 0.5850, Training Acc: 0.8471\n",
            "Cora - Epoch: 26, Loss: 0.5599, Training Acc: 0.8527\n",
            "Cora - Epoch: 27, Loss: 0.5450, Training Acc: 0.8527\n",
            "Cora - Epoch: 28, Loss: 0.5198, Training Acc: 0.8564\n",
            "Cora - Epoch: 29, Loss: 0.4954, Training Acc: 0.8545\n",
            "Cora - Epoch: 30, Loss: 0.4798, Training Acc: 0.8545\n",
            "Cora - Epoch: 31, Loss: 0.4805, Training Acc: 0.8508\n",
            "Cora - Epoch: 32, Loss: 0.4398, Training Acc: 0.8656\n",
            "Cora - Epoch: 33, Loss: 0.4381, Training Acc: 0.8674\n",
            "Cora - Epoch: 34, Loss: 0.4316, Training Acc: 0.8674\n",
            "Cora - Epoch: 35, Loss: 0.4187, Training Acc: 0.8674\n",
            "Cora - Epoch: 36, Loss: 0.4109, Training Acc: 0.8637\n",
            "Cora - Epoch: 37, Loss: 0.4122, Training Acc: 0.8619\n",
            "Cora - Epoch: 38, Loss: 0.3941, Training Acc: 0.8692\n",
            "Cora - Epoch: 39, Loss: 0.3797, Training Acc: 0.8674\n",
            "Cora - Epoch: 40, Loss: 0.3865, Training Acc: 0.8711\n",
            "Cora - Epoch: 41, Loss: 0.3801, Training Acc: 0.8729\n",
            "Cora - Epoch: 42, Loss: 0.3684, Training Acc: 0.8785\n",
            "Cora - Epoch: 43, Loss: 0.3689, Training Acc: 0.8803\n",
            "Cora - Epoch: 44, Loss: 0.3580, Training Acc: 0.8785\n",
            "Cora - Epoch: 45, Loss: 0.3476, Training Acc: 0.8729\n",
            "Cora - Epoch: 46, Loss: 0.3640, Training Acc: 0.8766\n",
            "Cora - Epoch: 47, Loss: 0.3620, Training Acc: 0.8766\n",
            "Cora - Epoch: 48, Loss: 0.3579, Training Acc: 0.8729\n",
            "Cora - Epoch: 49, Loss: 0.3467, Training Acc: 0.8711\n",
            "Cora - Epoch: 50, Loss: 0.3355, Training Acc: 0.8711\n",
            "Cora - Epoch: 51, Loss: 0.3333, Training Acc: 0.8711\n",
            "Cora - Epoch: 52, Loss: 0.3352, Training Acc: 0.8711\n",
            "Cora - Epoch: 53, Loss: 0.3272, Training Acc: 0.8748\n",
            "Cora - Epoch: 54, Loss: 0.3333, Training Acc: 0.8748\n",
            "Cora - Epoch: 55, Loss: 0.3247, Training Acc: 0.8748\n",
            "Cora - Epoch: 56, Loss: 0.3320, Training Acc: 0.8785\n",
            "Cora - Epoch: 57, Loss: 0.3156, Training Acc: 0.8766\n",
            "Cora - Epoch: 58, Loss: 0.3187, Training Acc: 0.8729\n",
            "Cora - Epoch: 59, Loss: 0.3154, Training Acc: 0.8748\n",
            "Cora - Epoch: 60, Loss: 0.3175, Training Acc: 0.8729\n",
            "Cora - Epoch: 61, Loss: 0.3153, Training Acc: 0.8748\n",
            "Cora - Epoch: 62, Loss: 0.3208, Training Acc: 0.8748\n",
            "Cora - Epoch: 63, Loss: 0.3131, Training Acc: 0.8729\n",
            "Cora - Epoch: 64, Loss: 0.3213, Training Acc: 0.8748\n",
            "Cora - Epoch: 65, Loss: 0.3101, Training Acc: 0.8711\n",
            "Cora - Epoch: 66, Loss: 0.3060, Training Acc: 0.8748\n",
            "Cora - Epoch: 67, Loss: 0.3139, Training Acc: 0.8729\n",
            "Cora - Epoch: 68, Loss: 0.3082, Training Acc: 0.8729\n",
            "Cora - Epoch: 69, Loss: 0.3054, Training Acc: 0.8729\n",
            "Cora - Epoch: 70, Loss: 0.3084, Training Acc: 0.8729\n",
            "Cora - Epoch: 71, Loss: 0.3012, Training Acc: 0.8729\n",
            "Cora - Epoch: 72, Loss: 0.3051, Training Acc: 0.8711\n",
            "Cora - Epoch: 73, Loss: 0.3113, Training Acc: 0.8711\n",
            "Cora - Epoch: 74, Loss: 0.2935, Training Acc: 0.8711\n",
            "Cora - Epoch: 75, Loss: 0.3006, Training Acc: 0.8729\n",
            "Cora - Epoch: 76, Loss: 0.3034, Training Acc: 0.8729\n",
            "Cora - Epoch: 77, Loss: 0.3051, Training Acc: 0.8711\n",
            "Cora - Epoch: 78, Loss: 0.2931, Training Acc: 0.8766\n",
            "Cora - Epoch: 79, Loss: 0.2963, Training Acc: 0.8729\n",
            "Cora - Epoch: 80, Loss: 0.2965, Training Acc: 0.8729\n",
            "Cora - Epoch: 81, Loss: 0.2933, Training Acc: 0.8656\n",
            "Cora - Epoch: 82, Loss: 0.3037, Training Acc: 0.8637\n",
            "Cora - Epoch: 83, Loss: 0.2906, Training Acc: 0.8674\n",
            "Cora - Epoch: 84, Loss: 0.2902, Training Acc: 0.8692\n",
            "Cora - Epoch: 85, Loss: 0.2983, Training Acc: 0.8711\n",
            "Cora - Epoch: 86, Loss: 0.2961, Training Acc: 0.8674\n",
            "Cora - Epoch: 87, Loss: 0.2848, Training Acc: 0.8656\n",
            "Cora - Epoch: 88, Loss: 0.2858, Training Acc: 0.8692\n",
            "Cora - Epoch: 89, Loss: 0.2930, Training Acc: 0.8692\n",
            "Cora - Epoch: 90, Loss: 0.2929, Training Acc: 0.8674\n",
            "Cora - Epoch: 91, Loss: 0.2849, Training Acc: 0.8656\n",
            "Cora - Epoch: 92, Loss: 0.2816, Training Acc: 0.8674\n",
            "Cora - Epoch: 93, Loss: 0.2862, Training Acc: 0.8692\n",
            "Cora - Epoch: 94, Loss: 0.2813, Training Acc: 0.8711\n",
            "Cora - Epoch: 95, Loss: 0.2887, Training Acc: 0.8748\n",
            "Cora - Epoch: 96, Loss: 0.2818, Training Acc: 0.8766\n",
            "Cora - Epoch: 97, Loss: 0.2772, Training Acc: 0.8766\n",
            "Cora - Epoch: 98, Loss: 0.2816, Training Acc: 0.8785\n",
            "Cora - Epoch: 99, Loss: 0.2778, Training Acc: 0.8766\n",
            "Cora - Epoch: 100, Loss: 0.2761, Training Acc: 0.8766\n",
            "Cora - Epoch: 101, Loss: 0.2750, Training Acc: 0.8748\n",
            "Cora - Epoch: 102, Loss: 0.2791, Training Acc: 0.8748\n",
            "Cora - Epoch: 103, Loss: 0.2778, Training Acc: 0.8766\n",
            "Cora - Epoch: 104, Loss: 0.2751, Training Acc: 0.8748\n",
            "Cora - Epoch: 105, Loss: 0.2717, Training Acc: 0.8729\n",
            "Cora - Epoch: 106, Loss: 0.2818, Training Acc: 0.8729\n",
            "Cora - Epoch: 107, Loss: 0.2736, Training Acc: 0.8674\n",
            "Cora - Epoch: 108, Loss: 0.2671, Training Acc: 0.8729\n",
            "Cora - Epoch: 109, Loss: 0.2737, Training Acc: 0.8692\n",
            "Cora - Epoch: 110, Loss: 0.2715, Training Acc: 0.8692\n",
            "Cora - Epoch: 111, Loss: 0.2688, Training Acc: 0.8711\n",
            "Cora - Epoch: 112, Loss: 0.2767, Training Acc: 0.8711\n",
            "Cora - Epoch: 113, Loss: 0.2693, Training Acc: 0.8729\n",
            "Cora - Epoch: 114, Loss: 0.2699, Training Acc: 0.8748\n",
            "Cora - Epoch: 115, Loss: 0.2662, Training Acc: 0.8748\n",
            "Cora - Epoch: 116, Loss: 0.2751, Training Acc: 0.8711\n",
            "Cora - Epoch: 117, Loss: 0.2756, Training Acc: 0.8766\n",
            "Cora - Epoch: 118, Loss: 0.2745, Training Acc: 0.8711\n",
            "Cora - Epoch: 119, Loss: 0.2645, Training Acc: 0.8711\n",
            "Cora - Epoch: 120, Loss: 0.2708, Training Acc: 0.8711\n",
            "Cora - Epoch: 121, Loss: 0.2661, Training Acc: 0.8711\n",
            "Cora - Epoch: 122, Loss: 0.2698, Training Acc: 0.8711\n",
            "Cora - Epoch: 123, Loss: 0.2734, Training Acc: 0.8711\n",
            "Cora - Epoch: 124, Loss: 0.2591, Training Acc: 0.8711\n",
            "Cora - Epoch: 125, Loss: 0.2691, Training Acc: 0.8711\n",
            "Cora - Epoch: 126, Loss: 0.2610, Training Acc: 0.8748\n",
            "Cora - Epoch: 127, Loss: 0.2628, Training Acc: 0.8748\n",
            "Cora - Epoch: 128, Loss: 0.2631, Training Acc: 0.8729\n",
            "Cora - Epoch: 129, Loss: 0.2661, Training Acc: 0.8748\n",
            "Cora - Epoch: 130, Loss: 0.2561, Training Acc: 0.8748\n",
            "Cora - Epoch: 131, Loss: 0.2671, Training Acc: 0.8729\n",
            "Cora - Epoch: 132, Loss: 0.2611, Training Acc: 0.8766\n",
            "Cora - Epoch: 133, Loss: 0.2656, Training Acc: 0.8766\n",
            "Cora - Epoch: 134, Loss: 0.2570, Training Acc: 0.8766\n",
            "Cora - Epoch: 135, Loss: 0.2600, Training Acc: 0.8748\n",
            "Cora - Epoch: 136, Loss: 0.2628, Training Acc: 0.8766\n",
            "Cora - Epoch: 137, Loss: 0.2570, Training Acc: 0.8748\n",
            "Cora - Epoch: 138, Loss: 0.2636, Training Acc: 0.8692\n",
            "Cora - Epoch: 139, Loss: 0.2610, Training Acc: 0.8674\n",
            "Cora - Epoch: 140, Loss: 0.2643, Training Acc: 0.8656\n",
            "Cora - Epoch: 141, Loss: 0.2588, Training Acc: 0.8748\n",
            "Cora - Epoch: 142, Loss: 0.2640, Training Acc: 0.8711\n",
            "Cora - Epoch: 143, Loss: 0.2574, Training Acc: 0.8729\n",
            "Cora - Epoch: 144, Loss: 0.2545, Training Acc: 0.8748\n",
            "Cora - Epoch: 145, Loss: 0.2558, Training Acc: 0.8748\n",
            "Cora - Epoch: 146, Loss: 0.2534, Training Acc: 0.8785\n",
            "Cora - Epoch: 147, Loss: 0.2529, Training Acc: 0.8766\n",
            "Cora - Epoch: 148, Loss: 0.2549, Training Acc: 0.8766\n",
            "Cora - Epoch: 149, Loss: 0.2540, Training Acc: 0.8748\n",
            "Cora - Epoch: 150, Loss: 0.2501, Training Acc: 0.8729\n",
            "Cora - Epoch: 151, Loss: 0.2480, Training Acc: 0.8729\n",
            "Cora - Epoch: 152, Loss: 0.2597, Training Acc: 0.8748\n",
            "Cora - Epoch: 153, Loss: 0.2608, Training Acc: 0.8729\n",
            "Cora - Epoch: 154, Loss: 0.2524, Training Acc: 0.8711\n",
            "Cora - Epoch: 155, Loss: 0.2548, Training Acc: 0.8729\n",
            "Cora - Epoch: 156, Loss: 0.2463, Training Acc: 0.8674\n",
            "Cora - Epoch: 157, Loss: 0.2465, Training Acc: 0.8637\n",
            "Cora - Epoch: 158, Loss: 0.2521, Training Acc: 0.8674\n",
            "Cora - Epoch: 159, Loss: 0.2484, Training Acc: 0.8692\n",
            "Cora - Epoch: 160, Loss: 0.2483, Training Acc: 0.8674\n",
            "Cora - Epoch: 161, Loss: 0.2503, Training Acc: 0.8748\n",
            "Cora - Epoch: 162, Loss: 0.2533, Training Acc: 0.8729\n",
            "Cora - Epoch: 163, Loss: 0.2515, Training Acc: 0.8711\n",
            "Cora - Epoch: 164, Loss: 0.2522, Training Acc: 0.8711\n",
            "Cora - Epoch: 165, Loss: 0.2593, Training Acc: 0.8711\n",
            "Cora - Epoch: 166, Loss: 0.2441, Training Acc: 0.8692\n",
            "Cora - Epoch: 167, Loss: 0.2571, Training Acc: 0.8748\n",
            "Cora - Epoch: 168, Loss: 0.2481, Training Acc: 0.8766\n",
            "Cora - Epoch: 169, Loss: 0.2639, Training Acc: 0.8748\n",
            "Cora - Epoch: 170, Loss: 0.2447, Training Acc: 0.8766\n",
            "Cora - Epoch: 171, Loss: 0.2570, Training Acc: 0.8766\n",
            "Cora - Epoch: 172, Loss: 0.2512, Training Acc: 0.8766\n",
            "Cora - Epoch: 173, Loss: 0.2415, Training Acc: 0.8766\n",
            "Cora - Epoch: 174, Loss: 0.2509, Training Acc: 0.8748\n",
            "Cora - Epoch: 175, Loss: 0.2529, Training Acc: 0.8748\n",
            "Cora - Epoch: 176, Loss: 0.2475, Training Acc: 0.8748\n",
            "Cora - Epoch: 177, Loss: 0.2465, Training Acc: 0.8766\n",
            "Cora - Epoch: 178, Loss: 0.2483, Training Acc: 0.8748\n",
            "Cora - Epoch: 179, Loss: 0.2463, Training Acc: 0.8748\n",
            "Cora - Epoch: 180, Loss: 0.2436, Training Acc: 0.8748\n",
            "Cora - Epoch: 181, Loss: 0.2453, Training Acc: 0.8692\n",
            "Cora - Epoch: 182, Loss: 0.2454, Training Acc: 0.8692\n",
            "Cora - Epoch: 183, Loss: 0.2443, Training Acc: 0.8674\n",
            "Cora - Epoch: 184, Loss: 0.2358, Training Acc: 0.8674\n",
            "Cora - Epoch: 185, Loss: 0.2485, Training Acc: 0.8729\n",
            "Cora - Epoch: 186, Loss: 0.2425, Training Acc: 0.8729\n",
            "Cora - Epoch: 187, Loss: 0.2385, Training Acc: 0.8748\n",
            "Cora - Epoch: 188, Loss: 0.2466, Training Acc: 0.8729\n",
            "Cora - Epoch: 189, Loss: 0.2381, Training Acc: 0.8766\n",
            "Cora - Epoch: 190, Loss: 0.2354, Training Acc: 0.8766\n",
            "Cora - Epoch: 191, Loss: 0.2443, Training Acc: 0.8748\n",
            "Cora - Epoch: 192, Loss: 0.2376, Training Acc: 0.8748\n",
            "Cora - Epoch: 193, Loss: 0.2348, Training Acc: 0.8748\n",
            "Cora - Epoch: 194, Loss: 0.2412, Training Acc: 0.8748\n",
            "Cora - Epoch: 195, Loss: 0.2378, Training Acc: 0.8803\n",
            "Cora - Epoch: 196, Loss: 0.2432, Training Acc: 0.8766\n",
            "Cora - Epoch: 197, Loss: 0.2396, Training Acc: 0.8766\n",
            "Cora - Epoch: 198, Loss: 0.2320, Training Acc: 0.8729\n",
            "Cora - Epoch: 199, Loss: 0.2423, Training Acc: 0.8711\n",
            "Cora - Epoch: 200, Loss: 0.2380, Training Acc: 0.8729\n",
            "CiteSeer - Epoch: 1, Loss: 1.8067, Training Acc: 0.3598\n",
            "CiteSeer - Epoch: 2, Loss: 1.6996, Training Acc: 0.5142\n",
            "CiteSeer - Epoch: 3, Loss: 1.5541, Training Acc: 0.6372\n",
            "CiteSeer - Epoch: 4, Loss: 1.3931, Training Acc: 0.6987\n",
            "CiteSeer - Epoch: 5, Loss: 1.2362, Training Acc: 0.7151\n",
            "CiteSeer - Epoch: 6, Loss: 1.1269, Training Acc: 0.7376\n",
            "CiteSeer - Epoch: 7, Loss: 1.0118, Training Acc: 0.7421\n",
            "CiteSeer - Epoch: 8, Loss: 0.9337, Training Acc: 0.7511\n",
            "CiteSeer - Epoch: 9, Loss: 0.8665, Training Acc: 0.7556\n",
            "CiteSeer - Epoch: 10, Loss: 0.8070, Training Acc: 0.7511\n",
            "CiteSeer - Epoch: 11, Loss: 0.7747, Training Acc: 0.7481\n",
            "CiteSeer - Epoch: 12, Loss: 0.7246, Training Acc: 0.7511\n",
            "CiteSeer - Epoch: 13, Loss: 0.7003, Training Acc: 0.7571\n",
            "CiteSeer - Epoch: 14, Loss: 0.6725, Training Acc: 0.7571\n",
            "CiteSeer - Epoch: 15, Loss: 0.6493, Training Acc: 0.7616\n",
            "CiteSeer - Epoch: 16, Loss: 0.6295, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 17, Loss: 0.6062, Training Acc: 0.7676\n",
            "CiteSeer - Epoch: 18, Loss: 0.6106, Training Acc: 0.7646\n",
            "CiteSeer - Epoch: 19, Loss: 0.5922, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 20, Loss: 0.5794, Training Acc: 0.7691\n",
            "CiteSeer - Epoch: 21, Loss: 0.5660, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 22, Loss: 0.5494, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 23, Loss: 0.5262, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 24, Loss: 0.5257, Training Acc: 0.7661\n",
            "CiteSeer - Epoch: 25, Loss: 0.5260, Training Acc: 0.7661\n",
            "CiteSeer - Epoch: 26, Loss: 0.5253, Training Acc: 0.7691\n",
            "CiteSeer - Epoch: 27, Loss: 0.5120, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 28, Loss: 0.5000, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 29, Loss: 0.4903, Training Acc: 0.7751\n",
            "CiteSeer - Epoch: 30, Loss: 0.4807, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 31, Loss: 0.4865, Training Acc: 0.7691\n",
            "CiteSeer - Epoch: 32, Loss: 0.4801, Training Acc: 0.7676\n",
            "CiteSeer - Epoch: 33, Loss: 0.4759, Training Acc: 0.7676\n",
            "CiteSeer - Epoch: 34, Loss: 0.4605, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 35, Loss: 0.4600, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 36, Loss: 0.4594, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 37, Loss: 0.4633, Training Acc: 0.7736\n",
            "CiteSeer - Epoch: 38, Loss: 0.4647, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 39, Loss: 0.4487, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 40, Loss: 0.4529, Training Acc: 0.7751\n",
            "CiteSeer - Epoch: 41, Loss: 0.4500, Training Acc: 0.7751\n",
            "CiteSeer - Epoch: 42, Loss: 0.4515, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 43, Loss: 0.4480, Training Acc: 0.7736\n",
            "CiteSeer - Epoch: 44, Loss: 0.4425, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 45, Loss: 0.4395, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 46, Loss: 0.4378, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 47, Loss: 0.4373, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 48, Loss: 0.4329, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 49, Loss: 0.4325, Training Acc: 0.7736\n",
            "CiteSeer - Epoch: 50, Loss: 0.4286, Training Acc: 0.7691\n",
            "CiteSeer - Epoch: 51, Loss: 0.4289, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 52, Loss: 0.4218, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 53, Loss: 0.4289, Training Acc: 0.7781\n",
            "CiteSeer - Epoch: 54, Loss: 0.4303, Training Acc: 0.7781\n",
            "CiteSeer - Epoch: 55, Loss: 0.4232, Training Acc: 0.7781\n",
            "CiteSeer - Epoch: 56, Loss: 0.4216, Training Acc: 0.7796\n",
            "CiteSeer - Epoch: 57, Loss: 0.4230, Training Acc: 0.7781\n",
            "CiteSeer - Epoch: 58, Loss: 0.4188, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 59, Loss: 0.4240, Training Acc: 0.7781\n",
            "CiteSeer - Epoch: 60, Loss: 0.4145, Training Acc: 0.7811\n",
            "CiteSeer - Epoch: 61, Loss: 0.4209, Training Acc: 0.7796\n",
            "CiteSeer - Epoch: 62, Loss: 0.4169, Training Acc: 0.7796\n",
            "CiteSeer - Epoch: 63, Loss: 0.4161, Training Acc: 0.7736\n",
            "CiteSeer - Epoch: 64, Loss: 0.4110, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 65, Loss: 0.4129, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 66, Loss: 0.4010, Training Acc: 0.7736\n",
            "CiteSeer - Epoch: 67, Loss: 0.4030, Training Acc: 0.7751\n",
            "CiteSeer - Epoch: 68, Loss: 0.4144, Training Acc: 0.7736\n",
            "CiteSeer - Epoch: 69, Loss: 0.4061, Training Acc: 0.7751\n",
            "CiteSeer - Epoch: 70, Loss: 0.4100, Training Acc: 0.7751\n",
            "CiteSeer - Epoch: 71, Loss: 0.4011, Training Acc: 0.7781\n",
            "CiteSeer - Epoch: 72, Loss: 0.3988, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 73, Loss: 0.3991, Training Acc: 0.7751\n",
            "CiteSeer - Epoch: 74, Loss: 0.3946, Training Acc: 0.7781\n",
            "CiteSeer - Epoch: 75, Loss: 0.4003, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 76, Loss: 0.3917, Training Acc: 0.7811\n",
            "CiteSeer - Epoch: 77, Loss: 0.3979, Training Acc: 0.7781\n",
            "CiteSeer - Epoch: 78, Loss: 0.3912, Training Acc: 0.7781\n",
            "CiteSeer - Epoch: 79, Loss: 0.3976, Training Acc: 0.7751\n",
            "CiteSeer - Epoch: 80, Loss: 0.3937, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 81, Loss: 0.3939, Training Acc: 0.7691\n",
            "CiteSeer - Epoch: 82, Loss: 0.3976, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 83, Loss: 0.3926, Training Acc: 0.7676\n",
            "CiteSeer - Epoch: 84, Loss: 0.3925, Training Acc: 0.7676\n",
            "CiteSeer - Epoch: 85, Loss: 0.3887, Training Acc: 0.7751\n",
            "CiteSeer - Epoch: 86, Loss: 0.3881, Training Acc: 0.7781\n",
            "CiteSeer - Epoch: 87, Loss: 0.3850, Training Acc: 0.7796\n",
            "CiteSeer - Epoch: 88, Loss: 0.3968, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 89, Loss: 0.3793, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 90, Loss: 0.3816, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 91, Loss: 0.3876, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 92, Loss: 0.3847, Training Acc: 0.7796\n",
            "CiteSeer - Epoch: 93, Loss: 0.3797, Training Acc: 0.7796\n",
            "CiteSeer - Epoch: 94, Loss: 0.3785, Training Acc: 0.7736\n",
            "CiteSeer - Epoch: 95, Loss: 0.3811, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 96, Loss: 0.3858, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 97, Loss: 0.3835, Training Acc: 0.7796\n",
            "CiteSeer - Epoch: 98, Loss: 0.3737, Training Acc: 0.7796\n",
            "CiteSeer - Epoch: 99, Loss: 0.3682, Training Acc: 0.7811\n",
            "CiteSeer - Epoch: 100, Loss: 0.3737, Training Acc: 0.7811\n",
            "CiteSeer - Epoch: 101, Loss: 0.3783, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 102, Loss: 0.3732, Training Acc: 0.7736\n",
            "CiteSeer - Epoch: 103, Loss: 0.3698, Training Acc: 0.7736\n",
            "CiteSeer - Epoch: 104, Loss: 0.3720, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 105, Loss: 0.3715, Training Acc: 0.7751\n",
            "CiteSeer - Epoch: 106, Loss: 0.3764, Training Acc: 0.7796\n",
            "CiteSeer - Epoch: 107, Loss: 0.3713, Training Acc: 0.7751\n",
            "CiteSeer - Epoch: 108, Loss: 0.3726, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 109, Loss: 0.3727, Training Acc: 0.7751\n",
            "CiteSeer - Epoch: 110, Loss: 0.3700, Training Acc: 0.7751\n",
            "CiteSeer - Epoch: 111, Loss: 0.3675, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 112, Loss: 0.3726, Training Acc: 0.7631\n",
            "CiteSeer - Epoch: 113, Loss: 0.3714, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 114, Loss: 0.3654, Training Acc: 0.7691\n",
            "CiteSeer - Epoch: 115, Loss: 0.3677, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 116, Loss: 0.3680, Training Acc: 0.7751\n",
            "CiteSeer - Epoch: 117, Loss: 0.3664, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 118, Loss: 0.3583, Training Acc: 0.7676\n",
            "CiteSeer - Epoch: 119, Loss: 0.3680, Training Acc: 0.7661\n",
            "CiteSeer - Epoch: 120, Loss: 0.3662, Training Acc: 0.7676\n",
            "CiteSeer - Epoch: 121, Loss: 0.3586, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 122, Loss: 0.3707, Training Acc: 0.7751\n",
            "CiteSeer - Epoch: 123, Loss: 0.3671, Training Acc: 0.7751\n",
            "CiteSeer - Epoch: 124, Loss: 0.3612, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 125, Loss: 0.3729, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 126, Loss: 0.3670, Training Acc: 0.7661\n",
            "CiteSeer - Epoch: 127, Loss: 0.3635, Training Acc: 0.7691\n",
            "CiteSeer - Epoch: 128, Loss: 0.3667, Training Acc: 0.7646\n",
            "CiteSeer - Epoch: 129, Loss: 0.3655, Training Acc: 0.7661\n",
            "CiteSeer - Epoch: 130, Loss: 0.3617, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 131, Loss: 0.3620, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 132, Loss: 0.3568, Training Acc: 0.7826\n",
            "CiteSeer - Epoch: 133, Loss: 0.3517, Training Acc: 0.7811\n",
            "CiteSeer - Epoch: 134, Loss: 0.3580, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 135, Loss: 0.3624, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 136, Loss: 0.3545, Training Acc: 0.7751\n",
            "CiteSeer - Epoch: 137, Loss: 0.3603, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 138, Loss: 0.3587, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 139, Loss: 0.3533, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 140, Loss: 0.3610, Training Acc: 0.7781\n",
            "CiteSeer - Epoch: 141, Loss: 0.3592, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 142, Loss: 0.3559, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 143, Loss: 0.3577, Training Acc: 0.7781\n",
            "CiteSeer - Epoch: 144, Loss: 0.3508, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 145, Loss: 0.3544, Training Acc: 0.7736\n",
            "CiteSeer - Epoch: 146, Loss: 0.3596, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 147, Loss: 0.3484, Training Acc: 0.7691\n",
            "CiteSeer - Epoch: 148, Loss: 0.3593, Training Acc: 0.7691\n",
            "CiteSeer - Epoch: 149, Loss: 0.3660, Training Acc: 0.7751\n",
            "CiteSeer - Epoch: 150, Loss: 0.3519, Training Acc: 0.7796\n",
            "CiteSeer - Epoch: 151, Loss: 0.3463, Training Acc: 0.7796\n",
            "CiteSeer - Epoch: 152, Loss: 0.3465, Training Acc: 0.7736\n",
            "CiteSeer - Epoch: 153, Loss: 0.3499, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 154, Loss: 0.3464, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 155, Loss: 0.3488, Training Acc: 0.7736\n",
            "CiteSeer - Epoch: 156, Loss: 0.3508, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 157, Loss: 0.3505, Training Acc: 0.7841\n",
            "CiteSeer - Epoch: 158, Loss: 0.3485, Training Acc: 0.7871\n",
            "CiteSeer - Epoch: 159, Loss: 0.3402, Training Acc: 0.7811\n",
            "CiteSeer - Epoch: 160, Loss: 0.3490, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 161, Loss: 0.3478, Training Acc: 0.7781\n",
            "CiteSeer - Epoch: 162, Loss: 0.3489, Training Acc: 0.7841\n",
            "CiteSeer - Epoch: 163, Loss: 0.3462, Training Acc: 0.7826\n",
            "CiteSeer - Epoch: 164, Loss: 0.3464, Training Acc: 0.7796\n",
            "CiteSeer - Epoch: 165, Loss: 0.3433, Training Acc: 0.7781\n",
            "CiteSeer - Epoch: 166, Loss: 0.3554, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 167, Loss: 0.3472, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 168, Loss: 0.3520, Training Acc: 0.7691\n",
            "CiteSeer - Epoch: 169, Loss: 0.3533, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 170, Loss: 0.3478, Training Acc: 0.7631\n",
            "CiteSeer - Epoch: 171, Loss: 0.3545, Training Acc: 0.7676\n",
            "CiteSeer - Epoch: 172, Loss: 0.3532, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 173, Loss: 0.3398, Training Acc: 0.7736\n",
            "CiteSeer - Epoch: 174, Loss: 0.3468, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 175, Loss: 0.3435, Training Acc: 0.7691\n",
            "CiteSeer - Epoch: 176, Loss: 0.3424, Training Acc: 0.7661\n",
            "CiteSeer - Epoch: 177, Loss: 0.3415, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 178, Loss: 0.3374, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 179, Loss: 0.3357, Training Acc: 0.7796\n",
            "CiteSeer - Epoch: 180, Loss: 0.3336, Training Acc: 0.7811\n",
            "CiteSeer - Epoch: 181, Loss: 0.3457, Training Acc: 0.7841\n",
            "CiteSeer - Epoch: 182, Loss: 0.3433, Training Acc: 0.7811\n",
            "CiteSeer - Epoch: 183, Loss: 0.3472, Training Acc: 0.7796\n",
            "CiteSeer - Epoch: 184, Loss: 0.3431, Training Acc: 0.7736\n",
            "CiteSeer - Epoch: 185, Loss: 0.3409, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 186, Loss: 0.3389, Training Acc: 0.7736\n",
            "CiteSeer - Epoch: 187, Loss: 0.3303, Training Acc: 0.7676\n",
            "CiteSeer - Epoch: 188, Loss: 0.3408, Training Acc: 0.7691\n",
            "CiteSeer - Epoch: 189, Loss: 0.3417, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 190, Loss: 0.3422, Training Acc: 0.7721\n",
            "CiteSeer - Epoch: 191, Loss: 0.3391, Training Acc: 0.7691\n",
            "CiteSeer - Epoch: 192, Loss: 0.3436, Training Acc: 0.7691\n",
            "CiteSeer - Epoch: 193, Loss: 0.3338, Training Acc: 0.7661\n",
            "CiteSeer - Epoch: 194, Loss: 0.3468, Training Acc: 0.7661\n",
            "CiteSeer - Epoch: 195, Loss: 0.3437, Training Acc: 0.7706\n",
            "CiteSeer - Epoch: 196, Loss: 0.3444, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 197, Loss: 0.3418, Training Acc: 0.7751\n",
            "CiteSeer - Epoch: 198, Loss: 0.3382, Training Acc: 0.7736\n",
            "CiteSeer - Epoch: 199, Loss: 0.3376, Training Acc: 0.7766\n",
            "CiteSeer - Epoch: 200, Loss: 0.3468, Training Acc: 0.7766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import GCNConv, APPNP\n",
        "\n",
        "# Data loading and preprocessing\n",
        "def load_and_print_stats(dataset_name):\n",
        "    dataset = Planetoid(root=f'/tmp/{dataset_name}', name=dataset_name)\n",
        "    data = dataset[0]\n",
        "    print(f\"{dataset_name} Dataset Statistics:\")\n",
        "    print(f\"Number of Nodes: {data.num_nodes}\")\n",
        "    print(f\"Number of Edges: {data.num_edges}\")\n",
        "    print(f\"Number of Classes: {dataset.num_classes}\")\n",
        "    print(f\"Number of Node Features: {data.num_node_features}\")\n",
        "    return data, dataset.num_classes\n",
        "\n",
        "def split_data(data, dataset_name, train_ratio=0.7, val_ratio=0.1):\n",
        "    num_nodes = data.num_nodes\n",
        "    indices = np.random.permutation(num_nodes)\n",
        "\n",
        "    train_size = int(num_nodes * train_ratio)\n",
        "    val_size = int(num_nodes * val_ratio)\n",
        "\n",
        "    data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "    data.train_mask[indices[:train_size]] = True\n",
        "    data.val_mask[indices[train_size:train_size + val_size]] = True\n",
        "    data.test_mask[indices[train_size + val_size:]] = True\n",
        "\n",
        "    print(f\"Data Split for {dataset_name}:\")\n",
        "    print(f\"Training Nodes: {data.train_mask.sum().item()}\")\n",
        "    print(f\"Validation Nodes: {data.val_mask.sum().item()}\")\n",
        "    print(f\"Test Nodes: {data.test_mask.sum().item()}\\n\")\n",
        "\n",
        "# Model definition\n",
        "class PPNP(torch.nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(PPNP, self).__init__()\n",
        "        self.conv1 = GCNConv(num_features, 16)\n",
        "        self.conv2 = GCNConv(16, num_classes)\n",
        "        self.propagation = APPNP(K=10, alpha=0.1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.propagation(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train(model, data, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(model, data, mask):\n",
        "    model.eval()\n",
        "    out = model(data)\n",
        "    _, pred = out.max(dim=1)\n",
        "    correct = pred[mask].eq(data.y[mask]).sum().item()\n",
        "    acc = correct / int(mask.sum())\n",
        "    return acc\n",
        "\n",
        "# Training loop for Cora dataset\n",
        "for epoch in range(200):\n",
        "    cora_loss = train(cora_model, cora_data, cora_optimizer)\n",
        "\n",
        "    # Optional: Evaluate on validation set during training\n",
        "    # cora_val_acc = evaluate(cora_model, cora_data, cora_data.val_mask)\n",
        "    # print(f'Cora - Epoch: {epoch+1}, Validation Acc: {cora_val_acc:.4f}')\n",
        "\n",
        "# Evaluate on test set after training\n",
        "cora_test_acc = evaluate(cora_model, cora_data, cora_data.test_mask)\n",
        "print(f'\\nCora - Final Test Accuracy: {cora_test_acc:.4f}')\n",
        "\n",
        "# Repeat for CiteSeer dataset\n",
        "for epoch in range(200):\n",
        "    citeseer_loss = train(citeseer_model, citeseer_data, citeseer_optimizer)\n",
        "\n",
        "# Evaluate on test set after training\n",
        "citeseer_test_acc = evaluate(citeseer_model, citeseer_data, citeseer_data.test_mask)\n",
        "print(f'\\nCiteSeer - Final Test Accuracy: {citeseer_test_acc:.4f}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFG84fXSRs0N",
        "outputId": "b503b5e7-7b43-4b81-83c8-c3746dbc77ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cora - Final Test Accuracy: 0.8674\n",
            "\n",
            "CiteSeer - Final Test Accuracy: 0.7331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ModifiedPPNP\n",
        "## قسمت ز سوال 2"
      ],
      "metadata": {
        "id": "dUH8dBs6_LcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import GCNConv, APPNP\n",
        "import torch.optim as optim\n",
        "\n",
        "class ModifiedPPNP(torch.nn.Module):\n",
        "    def __init__(self, num_features, num_classes, hidden_dim=16, K=10, alpha=0.1):\n",
        "        super(ModifiedPPNP, self).__init__()\n",
        "        self.lin = torch.nn.Linear(num_features, hidden_dim)\n",
        "        self.conv1 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.W = torch.nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
        "        self.propagation = APPNP(K, alpha)\n",
        "        self.lin2 = torch.nn.Linear(hidden_dim, num_classes)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.xavier_uniform_(self.W.data)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.lin(x))\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = torch.matmul(x, self.W)\n",
        "        x = self.propagation(x, edge_index)\n",
        "        x = self.lin2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "def train(model, data, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(model, data):\n",
        "    model.eval()\n",
        "    out = model(data)\n",
        "    _, pred = out.max(dim=1)\n",
        "    correct = pred[data.test_mask].eq(data.y[data.test_mask]).sum().item()\n",
        "    acc = correct / int(data.test_mask.sum())\n",
        "    return acc\n",
        "\n",
        "# Load datasets\n",
        "datasets = {\n",
        "    'Cora': Planetoid(root='/tmp/Cora', name='Cora'),\n",
        "    'CiteSeer': Planetoid(root='/tmp/CiteSeer', name='CiteSeer')\n",
        "}\n",
        "\n",
        "for name, dataset in datasets.items():\n",
        "    data = dataset[0]\n",
        "    model = ModifiedPPNP(num_features=dataset.num_node_features, num_classes=dataset.num_classes)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, data, optimizer)\n",
        "        if epoch % 10 == 0:\n",
        "            acc = evaluate(model, data)\n",
        "            print(f'{name} Epoch {epoch}: Loss: {loss:.4f}, Test Acc: {acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nATx7BCmhm8t",
        "outputId": "420fdf5e-67c5-4d66-eb6c-d77afaa08e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cora Epoch 0: Loss: 1.9542, Test Acc: 0.0910\n",
            "Cora Epoch 10: Loss: 1.1919, Test Acc: 0.6070\n",
            "Cora Epoch 20: Loss: 0.2109, Test Acc: 0.7320\n",
            "Cora Epoch 30: Loss: 0.0148, Test Acc: 0.7470\n",
            "Cora Epoch 40: Loss: 0.0026, Test Acc: 0.7520\n",
            "Cora Epoch 50: Loss: 0.0017, Test Acc: 0.7570\n",
            "Cora Epoch 60: Loss: 0.0026, Test Acc: 0.7650\n",
            "Cora Epoch 70: Loss: 0.0041, Test Acc: 0.7790\n",
            "Cora Epoch 80: Loss: 0.0050, Test Acc: 0.7750\n",
            "Cora Epoch 90: Loss: 0.0048, Test Acc: 0.7800\n",
            "Cora Epoch 100: Loss: 0.0044, Test Acc: 0.7800\n",
            "Cora Epoch 110: Loss: 0.0041, Test Acc: 0.7850\n",
            "Cora Epoch 120: Loss: 0.0039, Test Acc: 0.7850\n",
            "Cora Epoch 130: Loss: 0.0038, Test Acc: 0.7850\n",
            "Cora Epoch 140: Loss: 0.0036, Test Acc: 0.7840\n",
            "Cora Epoch 150: Loss: 0.0034, Test Acc: 0.7880\n",
            "Cora Epoch 160: Loss: 0.0033, Test Acc: 0.7870\n",
            "Cora Epoch 170: Loss: 0.0032, Test Acc: 0.7860\n",
            "Cora Epoch 180: Loss: 0.0031, Test Acc: 0.7890\n",
            "Cora Epoch 190: Loss: 0.0030, Test Acc: 0.7870\n",
            "CiteSeer Epoch 0: Loss: 1.8007, Test Acc: 0.1600\n",
            "CiteSeer Epoch 10: Loss: 0.5826, Test Acc: 0.6430\n",
            "CiteSeer Epoch 20: Loss: 0.0752, Test Acc: 0.6120\n",
            "CiteSeer Epoch 30: Loss: 0.0138, Test Acc: 0.6090\n",
            "CiteSeer Epoch 40: Loss: 0.0044, Test Acc: 0.6190\n",
            "CiteSeer Epoch 50: Loss: 0.0043, Test Acc: 0.6190\n",
            "CiteSeer Epoch 60: Loss: 0.0057, Test Acc: 0.6250\n",
            "CiteSeer Epoch 70: Loss: 0.0060, Test Acc: 0.6270\n",
            "CiteSeer Epoch 80: Loss: 0.0053, Test Acc: 0.6260\n",
            "CiteSeer Epoch 90: Loss: 0.0045, Test Acc: 0.6300\n",
            "CiteSeer Epoch 100: Loss: 0.0040, Test Acc: 0.6250\n",
            "CiteSeer Epoch 110: Loss: 0.0037, Test Acc: 0.6250\n",
            "CiteSeer Epoch 120: Loss: 0.0034, Test Acc: 0.6250\n",
            "CiteSeer Epoch 130: Loss: 0.0032, Test Acc: 0.6250\n",
            "CiteSeer Epoch 140: Loss: 0.0030, Test Acc: 0.6260\n",
            "CiteSeer Epoch 150: Loss: 0.0028, Test Acc: 0.6240\n",
            "CiteSeer Epoch 160: Loss: 0.0027, Test Acc: 0.6250\n",
            "CiteSeer Epoch 170: Loss: 0.0026, Test Acc: 0.6250\n",
            "CiteSeer Epoch 180: Loss: 0.0025, Test Acc: 0.6220\n",
            "CiteSeer Epoch 190: Loss: 0.0025, Test Acc: 0.6230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import GCNConv, APPNP\n",
        "import torch.optim as optim\n",
        "\n",
        "class ModifiedPPNP(torch.nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super().__init__()\n",
        "        self.lin = torch.nn.Linear(num_features, 16)\n",
        "        self.conv1 = GCNConv(16, 16)\n",
        "        self.W = torch.nn.Parameter(torch.Tensor(16, 16))\n",
        "        self.propagation = APPNP(K=10, alpha=0.1)\n",
        "        self.lin2 = torch.nn.Linear(16, num_classes)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        torch.nn.init.xavier_uniform_(self.W.data)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.lin(x))\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = torch.matmul(x, self.W)\n",
        "        x = self.propagation(x, edge_index)\n",
        "        x = self.lin2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "def train(model, data, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def evaluate(model, data):\n",
        "    model.eval()\n",
        "    out = model(data)\n",
        "    _, pred = out.max(dim=1)\n",
        "    correct = pred[data.test_mask].eq(data.y[data.test_mask]).sum().item()\n",
        "    acc = correct / int(data.test_mask.sum())\n",
        "    return acc\n",
        "\n",
        "datasets = {'Cora': Planetoid(root='/tmp/Cora', name='Cora'), 'CiteSeer': Planetoid(root='/tmp/CiteSeer', name='CiteSeer')}\n",
        "\n",
        "for name, dataset in datasets.items():\n",
        "    data = dataset[0]\n",
        "    model = ModifiedPPNP(num_features=dataset.num_node_features, num_classes=dataset.num_classes)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "    for epoch in range(200):\n",
        "        loss = train(model, data, optimizer)\n",
        "    test_acc = evaluate(model, data)\n",
        "    print(f'{name} - Final Test Accuracy: {test_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3qotZ8JlDpY",
        "outputId": "36d61bdf-73a9-4ba9-b3df-27ec9314ce52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cora - Final Test Accuracy: 0.7670\n",
            "CiteSeer - Final Test Accuracy: 0.6180\n"
          ]
        }
      ]
    }
  ]
}